{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COSC-247 Ensemble Assignment\n",
    "\n",
    "Mia (Bomi) Jung\n",
    "\n",
    "Includes code from *Python Machine Learning 3rd Edition* by Sebastian Raschka, Packt Publishing Ltd. 2019\n",
    "\n",
    "Prepared for use in COSC-247 Machine Learning at Amherst College, Fall 2022, by Lee Spector (lspector@amherst.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Data\n",
    "The following dataset includes details of various applications scraped directly from Google play.\n",
    "\n",
    "This includes the name of app, the category of the app, the overall user rating of the app (as when scraped), the nmber of user reviews for the app (as when scraped), the number of user downloads/installs for the app (as when scraped), and much more.\n",
    "\n",
    "Data was attained from this web page:\n",
    "https://www.kaggle.com/datasets/lava18/google-play-store-apps?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>House party - live chat</th>\n",
       "      <th>DATING</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>9.2M</th>\n",
       "      <th>10.00</th>\n",
       "      <th>0</th>\n",
       "      <th>Mature 17+</th>\n",
       "      <th>Dating</th>\n",
       "      <th>31-Jul-18</th>\n",
       "      <th>3.52</th>\n",
       "      <th>4.0.3 and up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9360</th>\n",
       "      <td>Master E.K</td>\n",
       "      <td>FAMILY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90</td>\n",
       "      <td>Varies with device</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Education</td>\n",
       "      <td>11-Aug-17</td>\n",
       "      <td>1.5.0</td>\n",
       "      <td>4.4 and up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9361</th>\n",
       "      <td>Barisal University App-BU Face</td>\n",
       "      <td>FAMILY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100</td>\n",
       "      <td>10M</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Education</td>\n",
       "      <td>6-May-18</td>\n",
       "      <td>3.1.1</td>\n",
       "      <td>4.0.3 and up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9362</th>\n",
       "      <td>Oración CX</td>\n",
       "      <td>LIFESTYLE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>103</td>\n",
       "      <td>3.8M</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>12-Sep-17</td>\n",
       "      <td>5.1.10</td>\n",
       "      <td>4.1 and up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363</th>\n",
       "      <td>FD Calculator (EMI, SIP, RD &amp; Loan Eligilibility)</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>104</td>\n",
       "      <td>2.3M</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Finance</td>\n",
       "      <td>7-Aug-18</td>\n",
       "      <td>2.1.0</td>\n",
       "      <td>4.1 and up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9364</th>\n",
       "      <td>Ríos de Fe</td>\n",
       "      <td>LIFESTYLE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>141</td>\n",
       "      <td>15M</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>24-Mar-18</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.1 and up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                House party - live chat     DATING    1  1.1  \\\n",
       "9360                                         Master E.K     FAMILY  5.0   90   \n",
       "9361                     Barisal University App-BU Face     FAMILY  5.0  100   \n",
       "9362                                         Oración CX  LIFESTYLE  5.0  103   \n",
       "9363  FD Calculator (EMI, SIP, RD & Loan Eligilibility)    FINANCE  5.0  104   \n",
       "9364                                         Ríos de Fe  LIFESTYLE  5.0  141   \n",
       "\n",
       "                    9.2M   10.00  0 Mature 17+     Dating  31-Jul-18    3.52  \\\n",
       "9360  Varies with device  1000.0  0   Everyone  Education  11-Aug-17   1.5.0   \n",
       "9361                 10M  1000.0  0   Everyone  Education   6-May-18   3.1.1   \n",
       "9362                3.8M  5000.0  0   Everyone  Lifestyle  12-Sep-17  5.1.10   \n",
       "9363                2.3M  1000.0  0   Everyone    Finance   7-Aug-18   2.1.0   \n",
       "9364                 15M  1000.0  0   Everyone  Lifestyle  24-Mar-18     1.8   \n",
       "\n",
       "      4.0.3 and up  \n",
       "9360    4.4 and up  \n",
       "9361  4.0.3 and up  \n",
       "9362    4.1 and up  \n",
       "9363    4.1 and up  \n",
       "9364    4.1 and up  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv \n",
    "\n",
    "try:\n",
    "    data = pd.read_csv('googleplaystore.csv') \n",
    "    \n",
    "except HTTPError:\n",
    "    s = 'googleplaystore.csv'\n",
    "    print('From local path:', s)\n",
    "    data = pd.read_csv(s,\n",
    "                     header=None,\n",
    "                     encoding='utf-8')\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Classification task\n",
    "\n",
    "I will be training machine learning models to classify the data into two categories: apps that have more than 1 million downloads (category 1), and apps that have MORE than 1 million downloads (category 2). \n",
    "\n",
    "The models will use features user rating and user reviews to predict whether the given data falls into category 1 or 2 .\n",
    "\n",
    "Since this is real-world data, some data pre-processing had to be done- including getting rid of 1000 separators (commans) and + signs in the number of downloads, changing Strings to floats, and getting rid of entries that had NULL values for user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Extract the number of installations from the data- but remember the numbers are in Strings.\n",
    "yStr = data.iloc[0:10841,5].values\n",
    "# Convert the number of installations from String to float.\n",
    "yNums = [float(numString) for numString in yStr]\n",
    "\n",
    "# Apps that have 100000 (1 million) views or more will be classified as 1. Otherwise, 0.\n",
    "y=[]\n",
    "for num in yNums:\n",
    "    if (num>=1000000):\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "        \n",
    "# User rating and reviews (in columns 2 and 3) will be used as the predictor variables.\n",
    "X = data.iloc[0:, [2, 3]].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions of models\n",
    "\n",
    "For my pipelines, I first used a logistic regression model, which is a linear classification model that uses the sigmoid function (an activation function whose output is the probability of an example belonging to class 1) to make predictions.\n",
    "\n",
    "I also used a Decision Tree Classifier, which is a model that breaks down the data by making a decision based on a series of questions. Based on features of the training data, this model learns a series of questions to infer the class labels of the examples, and outputs axis-parallel decision boundaries.\n",
    "\n",
    "The third model I used was the K-neighbors classifer, which simply memorizes the training data as its \"learning\" process (aka \"lazy learning\"), and assigns the class labels by majority vote taken from the k-nearest neighbors of the data we want to classify.\n",
    "\n",
    "I utilized StandardScaler to transform my features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.77 Stdev: 0.014 [LogisticRegression]\n",
      "Accuracy: 0.94 Stdev: 0.009 [Decision tree]\n",
      "Accuracy: 0.94 Stdev: 0.011 [KNN]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipe1 = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression())\n",
    "\n",
    "pipe2 = make_pipeline(DecisionTreeClassifier(max_depth=6,\n",
    "                                             criterion='entropy',\n",
    "                                             random_state=0))\n",
    "\n",
    "pipe3 = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=20,\n",
    "                                                             p=3,\n",
    "                                                             metric='minkowski'))\n",
    "\n",
    "clf_labels = ['LogisticRegression', 'Decision tree', 'KNN']\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe1, pipe2, pipe3], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "    print(\"Accuracy: \" + str(round(scores.mean(), 2)) + \n",
    "          \" Stdev: \" + str(round(scores.std(), 3)) +\n",
    "          \" [\" + label + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desription of the ensemble method\n",
    "\n",
    "I am using the majority voiting ensemble method, which allows us to combine my three classification algorithms\n",
    "to build a stronger meta classifier that balances out individual classifers' weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 Stdev: 0.014 [LogisticRegression]\n",
      "Accuracy: 0.94 Stdev: 0.009 [Decision tree]\n",
      "Accuracy: 0.94 Stdev: 0.011 [KNN]\n",
      "Accuracy: 0.94 Stdev: 0.01 [Majority voting]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "mv_clf = VotingClassifier(estimators=[('p', pipe1), ('dt', pipe2), ('kn', pipe3)])\n",
    "\n",
    "clf_labels += ['Majority voting']\n",
    "all_clf = [pipe1, pipe2, pipe3, mv_clf]\n",
    "\n",
    "for clf, label in zip(all_clf, clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "    print(\"Accuracy: \" + str(round(scores.mean(), 2)) + \n",
    "          \" Stdev: \" + str(round(scores.std(), 3)) +\n",
    "          \" [\" + label + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross-validation result summary\n",
    "\n",
    "The results of cross-validation show that The Logistic Regression Model had an accuracy of 0.77, with a standard deviation of 0.014. The Decision Tree and K-nearest neighbor models both had an accuracy of 0.94, and the decision tree's result had a standard deviation of 0.009 while that of the K-nearest neighbor model was 0.11. \n",
    "\n",
    "Overall, the Majority voting's Accuracy is 0.94 with a standard deviation of 0.01. It seems that out of all the models, the logistic regression model had the lowest accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 644\n",
      "Out of a total of: 2810\n",
      "Accuracy: 0.7708185053380783\n"
     ]
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe1.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', np.array(y_test).shape[0])\n",
    "print('Accuracy:', pipe1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 170\n",
      "Out of a total of: 2810\n",
      "Accuracy: 0.9395017793594306\n"
     ]
    }
   ],
   "source": [
    "pipe2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe2.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', np.array(y_test).shape[0])\n",
    "print('Accuracy:', pipe2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 165\n",
      "Out of a total of: 2810\n",
      "Accuracy: 0.9412811387900356\n"
     ]
    }
   ],
   "source": [
    "pipe3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe3.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', np.array(y_test).shape[0])\n",
    "print('Accuracy:', pipe3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified test set examples: 173\n",
      "Out of a total of: 2810\n",
      "Accuracy: 0.9384341637010676\n"
     ]
    }
   ],
   "source": [
    "mv_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mv_clf.predict(X_test)\n",
    "print('Misclassified test set examples:', (y_test != y_pred).sum())\n",
    "print('Out of a total of:', np.array(y_test).shape[0])\n",
    "print('Accuracy:', mv_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of results from testing on the testing data\n",
    "\n",
    "The results attained from testing on the testing data are pretty consistent with the results from cross-validation. \n",
    "\n",
    "The logistic regression model, which had the lowest accuracy level according to the cross-validation resutls, had the highest number of misclassified test set examples: at a whopping 644 missclassified examples, compared to the approximately 165 to 175 missclassified examples resulting from all the other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
